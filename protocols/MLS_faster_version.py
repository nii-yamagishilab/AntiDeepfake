#!/usr/bin/env python
"""Faster script to create protocol for the Multilingual LibriSpeech (MLS) database
This script needs a file list generated by running:

find /path/to/your/MLS -name *.flac > MLS.txt

(inspired by https://github.com/gemelo-ai/vocos/tree/main)

Generating this MLS.txt filelist took ~3mins in our server head node, should be faster
in most machines. It looks like this:
/my/base_path/Data/MLS/mls_dutch/train/audio/5764/4370/5764_4370_000023.flac
/my/base_path/Data/MLS/mls_dutch/train/audio/5764/4370/5764_4370_000049.flac
/my/base_path/Data/MLS/mls_dutch/train/audio/5764/4370/5764_4370_000001.flac

Now, we can skip scanning the whole MLS folder with python os.walk(),
and use parallel_apply() to speed up.
You are encouraged to apply similar modifications for generating protocols for 
other large datasets, such as VoxCeleb or your own vocoded datasets.

no fake audios in this database

/path/to/your/MLS/
├── mls_dutch/
│   ├── dev/
│   │   ├── audio/../../xx.flac
│   ├── test/
│   │   ├── audio/../../xx.flac
│   ├── train/
│   │   ├── audio/../../xx.flac
├── mls_english/
├── . . . 
├── 

MLS.csv:
"""
import os
import csv

try:
    import pandas as pd
    from pandarallel import pandarallel
    import torchaudio
except ImportError:
    print("Please install pandas, pandarallel and torchaudio")
    sys.exit(1)


__author__ = "Wanying Ge, Xin Wang"
__email__ = "gewanying@nii.ac.jp, wangxin@nii.ac.jp"
__copyright__ = "Copyright 2025, National Institute of Informatics"

# used for pandas pd.parallel_apply() to speed up
pandarallel.initialize()

# Define paths
root_folder = '/gs/bs/tgh-25IAB/gewanying/Data/'
dataset_name = 'MLS'
data_folder = os.path.join(root_folder, dataset_name)
protocol_file = dataset_name + '.txt'
ID_PREFIX = 'MLS-'
output_csv = dataset_name + '.csv'

# Function to read the protocol file
def read_protocol(protocol_file):
    # define converter
    metadata = pd.read_csv(
        protocol_file,
        sep=' ',
        header=None,
        names=['ID'],
    )
    print(metadata.head())
    return metadata

# Function to collect additional metadata (duration and sample rate)
def collect_audio_metadata(metadata, root_folder):
    def __get_audio_meta(row):
        file_path = row['ID']
        if os.path.exists(file_path):
            metainfo = torchaudio.info(file_path)
            sample_rate = metainfo.sample_rate
            num_channels = metainfo.num_channels
            duration = round(metainfo.num_frames / sample_rate, 2)
            filepath = file_path.replace(root_folder, "$ROOT/")
            parts = os.path.normpath(filepath).split(os.sep)
# ['$ROOT', 'MLS', 'mls_english', 'train', 'audio', '6249', '9965', '6249_9965_001469.flac']
            subset = parts[3]
            if 'train' in subset:
                proportion = 'train'
            elif 'dev' in subset:
                proportion = 'valid'
            elif 'test' in subset:
                proportion = 'test'
            lang_id = parts[2]
            if 'italian' in lang_id:
                language = 'IT'
            elif 'dutch' in lang_id:
                language = 'NL'
            elif 'english' in lang_id:
                language = 'EN'
            elif 'french' in lang_id:
                language = 'FR'
            elif 'german' in lang_id:
                language = 'DE'
            elif 'spanish' in lang_id:
                language = 'ES'
            speaker = parts[5]
            label = 'real'
            attack = '-'
            encoding = metainfo.encoding
            bitpersample = metainfo.bits_per_sample
            file_id = os.path.basename(file_path)
        else:
            print(f"Warning: File {file_path} does not exist, skipping entry.")
            duration = -1
            sample_rate = -1
            filepath = ""
            encoding = ""
            bitpersample = -1
            num_channels = -1
            speaker = ""
            language = ""
            proportion = ""
            attack = ""
            label = ""
            file_id = ""
        row["ID"] = ID_PREFIX + file_id
        row["Attack"] = attack
        row["Speaker"] = speaker
        row['Label'] = label
        row["Duration"] = duration
        row["SampleRate"] = sample_rate
        row["Path"] = filepath
        row["Proportion"] = proportion
        row["AudioChannel"] = num_channels
        row["AudioEncoding"] = encoding
        row["AudioBitSample"] = bitpersample
        row["Language"] = language 
        return row 

    metadata = metadata.parallel_apply(lambda x: __get_audio_meta(x), axis=1)
    return metadata

# Write to CSV
def write_csv(metadata):
    header = ["ID", "Label", "Duration", "SampleRate", "Path", "Attack", "Speaker",\
              "Proportion", "AudioChannel", "AudioEncoding", "AudioBitSample",\
              "Language"]
    metadata = pd.DataFrame(metadata)
    metadata = metadata[header]
    metadata.to_csv(output_csv, index=False)

# Main script
if __name__ == "__main__":
    # Step 1: Read protocol and collect initial metadata
    metadata = read_protocol(protocol_file)
    # Step 2: Collect audio metadata (duration, sample rate, etc.)
    metadata = collect_audio_metadata(metadata, root_folder)
    metadata = metadata.to_dict(orient='records')
    # Step 3: Write metadata to CSV
    write_csv(metadata)
    print(f"Metadata CSV written to {output_csv}")

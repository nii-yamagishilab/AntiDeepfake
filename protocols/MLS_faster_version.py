#!/usr/bin/env python
"""Faster script to create protocol for the Multilingual LibriSpeech (MLS) database
This script needs a file list generated by running:

find /path/to/your/MLS -name *.flac > MLS.txt

(inspired by https://github.com/gemelo-ai/vocos/tree/main)

Generating this MLS.txt filelist took ~3mins in our server head node, should be faster
in most machines. It looks like this:
/my/base_path/Data/MLS/mls_dutch/train/audio/5764/4370/5764_4370_000023.flac
/my/base_path/Data/MLS/mls_dutch/train/audio/5764/4370/5764_4370_000049.flac
/my/base_path/Data/MLS/mls_dutch/train/audio/5764/4370/5764_4370_000001.flac

Now, we can skip scanning the whole MLS folder with python os.walk().
You are encouraged to apply similar modifications for generating protocols for 
other large datasets, such as VoxCeleb or your own vocoded datasets.

no fake audios in this database

/path/to/your/MLS/
├── mls_dutch/
│   ├── dev/
│   │   ├── audio/../../xx.flac
│   ├── test/
│   │   ├── audio/../../xx.flac
│   ├── train/
│   │   ├── audio/../../xx.flac
├── mls_english/
├── . . . 
├── 

MLS.csv:
"""

import os
import csv
import torchaudio

try:
    import pandas as pd
    from pandarallel import pandarallel
    import torchaudio
except ImportError:
    print("Please install pandas, pandarallel and torchaudio")
    sys.exit(1)


__author__ = "Wanying Ge, Xin Wang"
__email__ = "gewanying@nii.ac.jp, wangxin@nii.ac.jp"
__copyright__ = "Copyright 2025, National Institute of Informatics"

# used for pandas pd.parallel_apply() to speed up
pandarallel.initialize()

# Define paths
root_folder = '/gs/bs/tgh-25IAB/gewanying/Data/'
dataset_name = 'MLS'
data_folder = os.path.join(root_folder, dataset_name)
protocol_file = dataset_name + '.txt'
ID_PREFIX = 'MLS-'
output_csv = dataset_name + '.csv'

# Function to read the protocol file
def read_protocol(protocol_file):
    # define converter
    metadata = pd.read_csv(
        protocol_file,
        sep=' ',
        header=None,
        names=['ID'],
    )
    print(metadata.head())
    return metadata

# Function to collect additional metadata (duration and sample rate)
def collect_audio_metadata(metadata, root_folder):
    def __get_audio_meta(row):
        file_path = os.path.join(data_folder, f"{row['ID']}")
        if os.path.exists(file_path):
            metainfo = torchaudio.info(file_path)
            sample_rate = metainfo.sample_rate
            num_channels = metainfo.num_channels
            duration = round(metainfo.num_frames / sample_rate, 2)
            filepath = file_path.replace(root_folder, "$ROOT/")
            parts = os.path.normpath(filepath).split(os.sep)
# ['$ROOT', 'MLS', 'mls_english', 'train', 'audio', '6249', '9965', '6249_9965_001469.flac']
            subset = parts[3]
            if 'train' in subset:
                    proportion = 'train'
            elif 'dev' in subset:
                proportion = 'valid'
            elif 'test' in subset:
                proportion = 'test'
            lang_id = parts[2]
            if 'italian' in lang_id:
                language = 'IT'
            elif 'dutch' in lang_id:
                language = 'NL'
            elif 'english' in lang_id:
                language = 'EN'
            elif 'french' in lang_id:
                language = 'FR'
            elif 'german' in lang_id:
                language = 'DE'
            elif 'spanish' in lang_id:
                language = 'ES'
            speaker = parts[5]
            label = 'real'
            attack = '-'
            encoding = metainfo.encoding
            bitpersample = metainfo.bits_per_sample
            file_id = os.path.basename(file_path)
        else:
            print(f"Warning: File {file_path} does not exist, skipping entry.")
            duration = -1
            sample_rate = -1
            filepath = ""
            encoding = ""
            bitpersample = -1
            num_channels = -1
            speaker = ""
            language = ""
            proportion = ""
            attack = ""
            label = ""
            file_id = ""
        row["ID"] = ID_PREFIX + file_id
        row["Attack"] = attack
        row["Speaker"] = speaker
        row['Label'] = label
        row["Duration"] = duration
        row["SampleRate"] = sample_rate
        row["Path"] = filepath
        row["Proportion"] = proportion 
        row["AudioChannel"] = num_channels
        row["AudioEncoding"] = encoding
        row["AudioBitSample"] = bitpersample
        row["Language"] = language 
        return row 

    metadata = metadata.parallel_apply(lambda x: __get_audio_meta(x), axis=1)
    return metadata

# Write to CSV
def write_csv(metadata):
    header = ["ID", "Label", "Duration", "SampleRate", "Path", "Attack", "Speaker",\
              "Proportion", "AudioChannel", "AudioEncoding", "AudioBitSample",\
              "Language"]
    metadata = pd.DataFrame(metadata)
    metadata = metadata[header]
    metadata.to_csv(output_csv, index=False)

# Main script
if __name__ == "__main__":
    # Step 1: Read protocol and collect initial metadata
    metadata = read_protocol(protocol_file)
    # Step 2: Collect audio metadata (duration, sample rate, etc.)
    metadata = collect_audio_metadata(metadata, root_folder)
    metadata = metadata.to_dict(orient='records')
    # Step 3: Write metadata to CSV
    write_csv(metadata)
    print(f"Metadata CSV written to {output_csv}")
